{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25d66d3",
   "metadata": {},
   "source": [
    "# 1. Translating EXIST training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9356e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test_case  id   source language  \\\n",
      "0  EXIST2021   1  twitter       en   \n",
      "1  EXIST2021   2  twitter       en   \n",
      "2  EXIST2021   3  twitter       en   \n",
      "3  EXIST2021   4  twitter       en   \n",
      "4  EXIST2021   5  twitter       en   \n",
      "\n",
      "                                                text       task1  \\\n",
      "0  She calls herself \"anti-feminazi\" how about sh...      sexist   \n",
      "1  Now, back to these women, the brave and the be...  non-sexist   \n",
      "2  @CurvyBandida @Xalynne_B Wow, your skirt is ve...      sexist   \n",
      "3  @AurelieGuiboud Incredible!  Beautiful!But I l...  non-sexist   \n",
      "4  i find it extremely hard to believe that kelly...  non-sexist   \n",
      "\n",
      "                    task2  \n",
      "0  ideological-inequality  \n",
      "1              non-sexist  \n",
      "2         objectification  \n",
      "3              non-sexist  \n",
      "4              non-sexist  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6977 entries, 0 to 6976\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   test_case  6977 non-null   object\n",
      " 1   id         6977 non-null   int64 \n",
      " 2   source     6977 non-null   object\n",
      " 3   language   6977 non-null   object\n",
      " 4   text       6977 non-null   object\n",
      " 5   task1      6977 non-null   object\n",
      " 6   task2      6977 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 381.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "EXISTtraining = pd.read_csv('EXIST2021_training.tsv', delimiter='\\t')\n",
    "\n",
    "print(EXISTtraining.head())\n",
    "\n",
    "print(EXISTtraining.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42e0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3411a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "833ff1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "260512f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets = EXISTtraining[EXISTtraining['language'] == 'es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3674308",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets = spanish_tweets.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f05ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, lang='en'):\n",
    "    try:\n",
    "        return translator.translate(text, dest=lang).text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d2a0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def safe_translate(text):\n",
    "    \"\"\"A safe translation function with error handling.\"\"\"\n",
    "    try:\n",
    "        return translate_text(text, 'en')\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9320c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets.loc[:, 'translated_text'] = spanish_tweets['text'].apply(safe_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1efcf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "3436  Nadie te va a tratar tan bien como un hombre q...   \n",
      "3437  @lindagisela74 Que rica putita obediente, afor...   \n",
      "3438  @BicireporteraDF Yo lo hice a los 18 años por ...   \n",
      "3439  las cosas q sueño son indicios de que yo enrea...   \n",
      "3440  Pero a la niña le gustó desde que lo vió, así ...   \n",
      "\n",
      "                                        translated_text  \n",
      "3436  Nobody is going to treat you as well as a man ...  \n",
      "3437  @lindagisela74 What a delicious obedient littl...  \n",
      "3438  @BicireporteraDF I did it when I was 18 on the...  \n",
      "3439  The things I dream are indications that I am a...  \n",
      "3440  But the girl liked it from the moment she saw ...  \n"
     ]
    }
   ],
   "source": [
    "print(spanish_tweets[['text', 'translated_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a8cffdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets translated: 3541\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tweets translated:\", spanish_tweets['translated_text'].notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68c1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTtraining.loc[spanish_tweets.index, 'text'] = spanish_tweets['translated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTtraining.to_csv('EXSITtraining_translated_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b7f13",
   "metadata": {},
   "source": [
    "# 2. Translating EXIST testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "607a81dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test_case    id   source language  \\\n",
      "0  EXIST2021  6978      gab       en   \n",
      "1  EXIST2021  6979  twitter       en   \n",
      "2  EXIST2021  6980  twitter       en   \n",
      "3  EXIST2021  6981  twitter       en   \n",
      "4  EXIST2021  6982  twitter       en   \n",
      "\n",
      "                                                text       task1  \\\n",
      "0  Pennsylvania State Rep horrifies with opening ...  non-sexist   \n",
      "1  @iilovegrapes He sounds like as ass, and very ...  non-sexist   \n",
      "2  @averyangryskel1 @4ARealistParty LOL! \"This be...      sexist   \n",
      "3  @WanderOrange @stalliontwink Rights?I mean yea...      sexist   \n",
      "4  the jack manifold appreciation i’m seeing is o...  non-sexist   \n",
      "\n",
      "                    task2  \n",
      "0              non-sexist  \n",
      "1              non-sexist  \n",
      "2  ideological-inequality  \n",
      "3  ideological-inequality  \n",
      "4              non-sexist  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4368 entries, 0 to 4367\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   test_case  4368 non-null   object\n",
      " 1   id         4368 non-null   int64 \n",
      " 2   source     4368 non-null   object\n",
      " 3   language   4368 non-null   object\n",
      " 4   text       4368 non-null   object\n",
      " 5   task1      4368 non-null   object\n",
      " 6   task2      4368 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 239.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EXISTtesting = pd.read_csv('EXISTtesting.tsv', delimiter='\\t')\n",
    "\n",
    "print(EXISTtesting.head())\n",
    "\n",
    "print(EXISTtesting.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1387475",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets2 = EXISTtesting[EXISTtesting['language'] == 'es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "257ac7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets2 = spanish_tweets2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0c478ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets2.loc[:, 'translated_text'] = spanish_tweets2['text'].apply(safe_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "311d2b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "2208                      Y te golpeo más fuerte perra.   \n",
      "2209  @wthxtme @GOLDENMXM Castigado tres meses sin s...   \n",
      "2210  @MaricaIndomito jajajaj el me he topado con ta...   \n",
      "2211  Resulta que hoy es el día de la juventud y no ...   \n",
      "2212  @EstefMolina_ Los que dicen que creen en la ig...   \n",
      "\n",
      "                                        translated_text  \n",
      "2208                        And I hit you harder bitch.  \n",
      "2209  @wthxtme @GOLDENMXM Grounded for three months ...  \n",
      "2210  @MaricaIndomito hahahaha I have come across so...  \n",
      "2211  It turns out that today is youth day and all I...  \n",
      "2212  @EstefMolina_ Those who say that they believe ...  \n"
     ]
    }
   ],
   "source": [
    "print(spanish_tweets2[['text', 'translated_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111879ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTtesting.loc[spanish_tweets2.index, 'text'] = spanish_tweets2['translated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTtesting.to_csv('EXISTtesting_translated_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ed5ea",
   "metadata": {},
   "source": [
    "# 3. Filtering and Translating G20 Dataset for Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "G20data = pd.read_csv('G20Hamburg.csv')\n",
    "print(G20data.info())\n",
    "print(G20data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "G20data = G20data.rename(columns={\n",
    "    'Tweet': 'TweetNumber',\n",
    "    'Tweet.1': 'TweetContent',\n",
    "})\n",
    "\n",
    "print(G20data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c737660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out retweets\n",
    "G20data = G20data[G20data['Is Retweet'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on tweet content\n",
    "G20data = G20data.drop_duplicates(subset=['TweetContent'])\n",
    "# Drop rows where the tweet content is missing\n",
    "G20data = G20data.dropna(subset=['TweetContent'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G20data.info())\n",
    "print(G20data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477131fb",
   "metadata": {},
   "source": [
    "## 3.1. Filtering G20 dataset for Phase 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# List of keywords and phrases related to Phase 1\n",
    "keywords = [\n",
    "    \"Ivanka Trump\", \"Ivanka\", \"Trump’s daughter\", \"Trump daughter\" \"Theresa May\", \"Theresa\", \"Theresa's\", \"Theresa May's\", \"Theresa May's\", \"UK Prime Minister\", \"UK PM\",\n",
    "    \"Erna Solberg\", \"Erna Solberg's\", \"Erna's\", \"Solberg's\", \"Erna\", \"Solberg\", \"Norwegian prime minister\", \"Norwegian PM\", \"Angela Merkel\", \"Angela\", \"Merkel\", \"Angela Merkel's\", \"Merkel's\", \"Angela's\",\n",
    "    \"Chancellor of Germany\", \"Chancellor\", \"Women\", \"women's\", \"woman\", \"woman's\", \"female\", \"female's\", \"girl\", \"girl's\", \"Gender\", \"Feminism\",\n",
    "    \"feminist\", \"'feminist's\" \"Women Entrepreneurs Finance Initiative\", \"WeFi\", \"We-fi\", \"women’s fund\",\n",
    "    \"Trump's Tochter\", \"Trump Tochter\" \"Premierminister des Vereinigten Königreichs\", \"UK Premierminister\",\n",
    "    \"Norwegischer Premierminister\", \"Bundeskanzlerin von Deutschland\", \"Bundeskanzlerin Deutschlands\",\n",
    "    \"Bundeskanzler\", \"Frauen\", \"für Frauen\", \"damen\",\"frau\", \"weiblich\", \"hündinnen\", \"weibchen\", \"mädchen\", \"mädels\", \"Geschlecht\", \"Feminismus\",\n",
    "    \"feministin\", \"feministinnen\", \"Finance Initiative für Unternehmerinnen\", \"Frauen-Unternehmerinnen-Finanzinitiative\",\n",
    "    \"Frauen Fonds\", \"Frauenrechte\", \"Unternehmerinnen\", \"Frauenförderung\", \"Gleichberechtigung\", \"Geschlechtergerechtigkeit\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_umlauts_and_specials(keyword):\n",
    "    replacements = {\n",
    "        'ä': '(ae|ä)',\n",
    "        'ö': '(oe|ö)',\n",
    "        'ü': '(ue|ü)',\n",
    "        'ß': '(ss|ß)'\n",
    "    }\n",
    "    for german_char, replacement in replacements.items():\n",
    "        keyword = re.sub(re.escape(german_char), replacement, keyword, flags=re.IGNORECASE)\n",
    "    return keyword\n",
    "\n",
    "def enhance_keyword_regex(keyword):\n",
    "    keyword = adjust_umlauts_and_specials(keyword)\n",
    "    \n",
    "    keyword = re.escape(keyword)\n",
    "    \n",
    "    keyword = keyword.replace(r'\\ ', r'\\s*')\n",
    "    \n",
    "    punctuations = [\"'\", \"-\", \".\"]  \n",
    "    for punct in punctuations:\n",
    "        keyword = keyword.replace(re.escape(punct), re.escape(punct) + '?')\n",
    "    \n",
    "    return keyword\n",
    "\n",
    "regex_pattern = r'\\b(' + '|'.join(enhance_keyword_regex(keyword) for keyword in keywords) + r')\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb08041",
   "metadata": {},
   "outputs": [],
   "source": [
    "G20data['matches'] = G20data['TweetContent'].str.contains(regex_pattern, case=False, na=False)\n",
    "filtered_G20data = G20data[G20data['matches']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) \n",
    "pd.set_option('display.max_rows', 500)  \n",
    "print(filtered_G20data.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fbe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_G20data['Tweet Language'].value_counts())\n",
    "\n",
    "print(G20data['Tweet Language'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_G20data.to_csv('G20_filtered_beforeT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5363a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G20_filtered_beforeT = pd.read_csv('G20_filtered_beforeT.csv')\n",
    "\n",
    "print(G20_filtered_beforeT.head())\n",
    "print(G20_filtered_beforeT.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each keyword\n",
    "keyword_counts = {keyword: 0 for keyword in keywords}\n",
    "\n",
    "for keyword in keywords:\n",
    "    keyword_regex = enhance_keyword_regex(keyword)\n",
    "    pattern = re.compile(r'\\b' + keyword_regex + r'\\b', re.IGNORECASE)\n",
    "    keyword_counts[keyword] = filtered_G20data['TweetContent'].apply(lambda x: bool(pattern.search(str(x)))).sum()\n",
    "\n",
    "keyword_counts_df = pd.DataFrame(keyword_counts.items(), columns=['Keyword', 'Count']).sort_values(by='Count', ascending=False)\n",
    "\n",
    "keyword_counts_df.to_csv('keyword_counts.csv', index=False)\n",
    "\n",
    "print(keyword_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98792b",
   "metadata": {},
   "source": [
    "## 3.2. Translating G20 dataset for Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f262ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G20data_filtered_Ger = filtered_G20data[filtered_G20data['Tweet Language'] == 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49820ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G20data_filtered_Ger = G20data_filtered_Ger.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "G20data_filtered_Ger.loc[:, 'translated_text'] = G20data_filtered_Ger['TweetContent'].apply(safe_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G20data_filtered_Ger[['TweetContent', 'translated_text']].head())\n",
    "print(G20data_filtered_Ger[['TweetContent', 'translated_text']].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G20translate['matches'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53919426",
   "metadata": {},
   "source": [
    "## 3.3. Merging translated tweets with English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4581aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = G20_filtered_beforeT.merge(\n",
    "    G20data_filtered_Ger[['UniqueID', 'translated_text']],\n",
    "    on='UniqueID',\n",
    "    how='left',\n",
    "    suffixes=('', '_translated')\n",
    ")\n",
    "\n",
    "merged_dataset['TweetContent'] = merged_dataset['translated_text'].combine_first(merged_dataset['TweetContent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10756566",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset.drop(columns=['translated_text'], inplace=True)\n",
    "\n",
    "merged_dataset.to_csv('G20_filtered_after_translation1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfcfd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original G20_filtered_beforeT data:\")\n",
    "print(G20_filtered_beforeT.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset2 = pd.read_csv('G20_filtered_after_translation1.csv')\n",
    "\n",
    "print(merged_dataset2.head())\n",
    "\n",
    "print(merged_dataset2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G20translate.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993d963",
   "metadata": {},
   "source": [
    "# 4. Preprocessing EXIST training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50fcff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "EXISTtraining = pd.read_csv('EXSITtraining_translated_dataset.csv')\n",
    "\n",
    "print(EXISTtraining.head())\n",
    "\n",
    "print(EXISTtraining.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTtesting = pd.read_csv('EXISTtesting_translated_dataset.csv')\n",
    "\n",
    "print(EXISTtesting.head())\n",
    "\n",
    "print(EXISTtesting.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e35449",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTtraining = EXISTtraining.drop_duplicates(subset='text', keep='first')\n",
    "\n",
    "print(EXISTtraining.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dbd004",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTtesting = EXISTtesting.drop_duplicates(subset='text', keep='first')\n",
    "\n",
    "print(EXISTtesting.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')  # For lemmatisation\n",
    "nltk.download('punkt')    # For tokenisation\n",
    "nltk.download('stopwords')  # For stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'http\\S+', 'URL', text)\n",
    "\n",
    "    text = re.sub(r'[^a-z\\s#@]', '', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = re.sub(r'#(\\w+)', r'<hashtag> \\1 </hashtag>', text)\n",
    "\n",
    "    text = re.sub(r'@(\\w+)', r'<mention> \\1 </mention>', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = text.split()\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    special_tags = {'<hashtag>', '</hashtag>', '<mention>', '</mention>'}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in special_tags:\n",
    "            lemmatized_tokens.append(token)\n",
    "        else:\n",
    "            if token not in stop_words and token.isalpha():\n",
    "                lemmatized_token = lemmatizer.lemmatize(token)\n",
    "                lemmatized_tokens.append(lemmatized_token)\n",
    "\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b267bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to training data\n",
    "EXISTtraining['cleaned_text'] = EXISTtraining['text'].apply(preprocess_text)\n",
    "EXISTtraining['lemmatized_text'] = EXISTtraining['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "print(EXISTtraining[['text', 'cleaned_text', 'lemmatized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdfe469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to testing data\n",
    "EXISTtesting['cleaned_text'] = EXISTtesting['text'].apply(preprocess_text)\n",
    "EXISTtesting['lemmatized_text'] = EXISTtesting['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "print(EXISTtesting[['text', 'cleaned_text', 'lemmatized_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c1f03",
   "metadata": {},
   "source": [
    "# 5. Preprocessing Phase 1 G20 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "G20data = pd.read_csv('G20_filtered_after_translation1.csv')\n",
    "\n",
    "print(G20data.head())\n",
    "\n",
    "print(G20data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to filtered G20 data\n",
    "G20data['cleaned_text'] = G20data['TweetContent'].apply(preprocess_text)\n",
    "G20data['lemmatized_text'] = G20data['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "print(G20data[['TweetContent', 'cleaned_text', 'lemmatized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e44c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "G20data.to_csv('G20data_preprocessed.csv', index=False)\n",
    "EXISTtesting.to_csv('EXISTtesting_preprocessed.csv', index=False)\n",
    "\n",
    "EXISTtraining.to_csv('EXISTtraining_preprocessed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409c7b5",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e81a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define keywords for each category\n",
    "categories = {\n",
    "    \"all_four\": [\n",
    "        \"Ivanka Trump\", \"Ivanka\", \"Trump’s daughter\", \"Trump daughter\", \"Ivanka's\", \n",
    "        \"Merkel\", \"Angela Merkel\", \"Merkel's\", \"Chancellor\", \"Angela Merkel's\", \"Angela's\", \n",
    "        \"Theresa May\", \"Theresa\", \"Theresa's\", \"Theresa May's\", \"Theresa May's\", \"UK Prime Minister\", \"UK PM\", \n",
    "        \"Erna Solberg\", \"Erna Solberg's\", \"Erna's\", \"Solberg's\", \"Erna\", \"Solberg\", \"Norwegian prime minister\", \"Norwegian PM\"\n",
    "    ],\n",
    "    \"ivanka_trump\": [\n",
    "        \"Ivanka Trump\", \"Ivanka\", \"Trump’s daughter\", \"Trump daughter\", \"Ivanka's\", \"Trump's Daughter:\"\n",
    "    ],\n",
    "    \"angela_merkel\": [\n",
    "        \"Merkel\", \"Angela Merkel\", \"Merkel's\", \"Chancellor\", \"Angela Merkel's\", \"Angela's\"\n",
    "    ],\n",
    "    \"theresa_may\": [\n",
    "        \"Theresa May\", \"Theresa\", \"Theresa's\", \"Theresa May's\", \"Theresa May's\", \"UK Prime Minister\", \"UK PM\", \"Mrs. May\", \"theresa_may\"\n",
    "    ],\n",
    "    \"erna_solberg\": [\n",
    "        \"Erna Solberg\", \"Erna Solberg's\", \"Erna's\", \"Solberg's\", \"Erna\", \"Solberg\", \"Norwegian prime minister\", \"Norwegian PM\"\n",
    "    ],\n",
    "    \"women\": [\n",
    "        \"Women\", \"women's\", \"woman\", \"woman's\", \"womens\", \"womans\", \"female\", \"females\", \"female's\", \"girl\", \"girl's\", \"Gender\", \"Feminism\", \"feminist\", \"'feminist's\", \"feminists\",\n",
    "    ],\n",
    "    \"we_fi\": [\n",
    "        \"Women Entrepreneurs Finance Initiative\", \"WeFi\", \"We-fi\", \"women’s fund\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def adjust_umlauts_and_specials(keyword):\n",
    "    replacements = {\n",
    "        'ä': '(ae|ä)',\n",
    "        'ö': '(oe|ö)',\n",
    "        'ü': '(ue|ü)',\n",
    "        'ß': '(ss|ß)'\n",
    "    }\n",
    "    for german_char, replacement in replacements.items():\n",
    "        keyword = re.sub(re.escape(german_char), replacement, keyword, flags=re.IGNORECASE)\n",
    "    return keyword\n",
    "\n",
    "def enhance_keyword_regex(keyword):\n",
    "    keyword = adjust_umlauts_and_specials(keyword)\n",
    "    \n",
    "    keyword = re.escape(keyword)\n",
    "    \n",
    "    keyword = keyword.replace(r'\\ ', r'\\s*')\n",
    "    \n",
    "    punctuations = [\"'\", \"-\", \".\"]  \n",
    "    for punct in punctuations:\n",
    "        keyword = keyword.replace(re.escape(punct), re.escape(punct) + '?')\n",
    "    \n",
    "    return keyword\n",
    "\n",
    "counts = {}\n",
    "for category, keywords in categories.items():\n",
    "    regex_pattern = r'\\b(' + '|'.join(enhance_keyword_regex(keyword) for keyword in keywords) + r')\\b'\n",
    "    G20data[category + '_matches'] = G20data['TweetContent'].str.contains(regex_pattern, case=False, na=False)\n",
    "    counts[category] = G20data[category + '_matches'].sum()\n",
    "\n",
    "for category, count in counts.items():\n",
    "    print(f\"Number of tweets in category '{category}': {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "categories = ['Angela Merkel', 'Ivanka Trump', 'Theresa May', 'Erna Solberg', 'General Discussion on Women', 'We-Fi', 'All Four Female Politicians']\n",
    "values = [4700, 2557, 286, 11, 960, 30, 7440]\n",
    "\n",
    "df_new = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Count': values\n",
    "})\n",
    "\n",
    "df_sorted_new = df_new.sort_values('Count', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "colors = ['royalblue' if cat != 'All Four Female Politicians' else 'darksalmon' for cat in df_sorted_new['Category']]\n",
    "bar_plot_new = sns.barplot(\n",
    "    x='Category', \n",
    "    y='Count', \n",
    "    data=df_sorted_new, \n",
    "    palette=colors\n",
    ")\n",
    "\n",
    "for p in bar_plot_new.patches:\n",
    "    height = p.get_height()    \n",
    "    plt.text(p.get_x() + p.get_width() / 2,  \n",
    "             height + 50, \n",
    "             '{:1.0f}'.format(height),  \n",
    "             ha='center', va='bottom', fontsize=15) \n",
    "\n",
    "def wrap_labels(labels, width=15):\n",
    "    return [textwrap.fill(label, width) for label in labels]\n",
    "\n",
    "wrapped_labels = wrap_labels(df_sorted_new['Category'], width=15)\n",
    "bar_plot_new.set_xticks(range(len(wrapped_labels))) \n",
    "bar_plot_new.set_xticklabels(wrapped_labels, fontsize=15, ha='center')\n",
    "\n",
    "for label in bar_plot_new.get_xticklabels():\n",
    "    label.set_rotation(0)  \n",
    "    label.set_ha('center')  \n",
    "    label.set_va('bottom') \n",
    "    label.set_y(-0.1)  \n",
    "\n",
    "plt.subplots_adjust(bottom=0.25)  \n",
    "\n",
    "plt.title('Mentions Related to Women', fontsize=17)\n",
    "plt.xlabel('Categories', fontsize=17, labelpad=20)  \n",
    "plt.ylabel('Number of Mentions', fontsize=17)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
